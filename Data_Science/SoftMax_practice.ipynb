{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bZh_A4iCXIPf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.FloatTensor([[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1,2,5,6], [1,6, 6, 6], [1, 7, 7, 7]])\n",
        "y_train = torch.FloatTensor([[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn(4, 3, requires_grad = True)\n",
        "b = torch.randn(1, 3, requires_grad = True)\n",
        "optimizer = torch.optim.Adam([W, b], lr = 0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "  # softmax hypothsis\n",
        "  hypo = torch.softmax(torch.mm(x_train, W) + b, dim = 1) # dim = 1 각 벡터(리스트)마다 연산\n",
        "  cost = -(y_train * torch.log(hypo)).sum(dim = 1).mean() # cross entropy\n",
        "\n",
        "  optimizer.zero_grad() # with torhc.no_grad() W.grad = 0, b.grad = 0\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 300 == 0:\n",
        "      print(f\"epoch: {epoch}, cost: {cost.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5jYpSmAYZDH",
        "outputId": "99f196a6-c885-429d-9273-dc7ec9a0a6fb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 8.532617568969727\n",
            "epoch: 300, cost: 0.19016045331954956\n",
            "epoch: 600, cost: 0.09715739637613297\n",
            "epoch: 900, cost: 0.0584617480635643\n",
            "epoch: 1200, cost: 0.03868718072772026\n",
            "epoch: 1500, cost: 0.027216758579015732\n",
            "epoch: 1800, cost: 0.019984988495707512\n",
            "epoch: 2100, cost: 0.015143690630793571\n",
            "epoch: 2400, cost: 0.011752012185752392\n",
            "epoch: 2700, cost: 0.009290091693401337\n",
            "epoch: 3000, cost: 0.007451973855495453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.requires_grad_(False)\n",
        "b.requires_grad_(False)\n",
        "\n",
        "x_test = torch.FloatTensor([[1, 11, 10, 9], [1, 3, 4, 3], [1, 1, 0, 1]])\n",
        "h = (torch.mm(x_test, W) + b).softmax(dim = 1)\n",
        "test_all = torch.softmax(h, dim = 1)\n",
        "# argmax = 해당 수식이 최대가 되는 값을 찾음\n",
        "print(test_all)\n",
        "print(torch.argmax(test_all, dim = 1))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quZHcJij0Iyi",
        "outputId": "774b0de7-eb94-414b-8bb7-14f03c546098"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5761, 0.2119, 0.2119],\n",
            "        [0.2344, 0.4528, 0.3128],\n",
            "        [0.2119, 0.2119, 0.5761]])\n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
        "model = nn.Linear(4, 3) # shape 와 동일\n",
        "W = torch.randn(4, 3, requires_grad = True)\n",
        "b = torch.randn(1, 3, requires_grad = True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
        "\n",
        "for epoch in range(3001):\n",
        "  # softmax hypothsis\n",
        "  h = model(x_train)\n",
        "  cost = F.cross_entropy(h, y_train)\n",
        "  optimizer.zero_grad() # with torhc.no_grad() W.grad = 0, b.grad = 0\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    if epoch % 300 == 0:\n",
        "      print(f\"epoch: {epoch}, cost: {cost.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp_IeXuOYmND",
        "outputId": "08e83bf3-f3c5-4a33-a5ee-67b361335071"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, cost: 2.663395881652832\n",
            "epoch: 300, cost: 0.14020684361457825\n",
            "epoch: 600, cost: 0.06037745624780655\n",
            "epoch: 900, cost: 0.03343347832560539\n",
            "epoch: 1200, cost: 0.02111174911260605\n",
            "epoch: 1500, cost: 0.014447448775172234\n",
            "epoch: 1800, cost: 0.010429644957184792\n",
            "epoch: 2100, cost: 0.00781738106161356\n",
            "epoch: 2400, cost: 0.0060229068621993065\n",
            "epoch: 2700, cost: 0.004737867042422295\n",
            "epoch: 3000, cost: 0.0037874681875109673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = torch.FloatTensor([[1, 11, 10, 9], [1, 3, 4, 3], [1, 1, 0, 1]])\n",
        "h = model(x_test)\n",
        "print(torch.argmax(h, dim = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF6CJjUs4muy",
        "outputId": "1ead1ed3-5865-4160-905d-abc866c6817b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x_train = np.array([[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1,7 ,5 ,5], [1,2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]])\n",
        "y_train = np.array([2, 2, 2, 1, 1, 1, 0, 0])\n",
        "\n",
        "logistic = LogisticRegression()\n",
        "logistic.fit(x_train, y_train)\n",
        "\n",
        "pred = logistic.predict([[1, 11, 10, 9], [1, 3, 4, 3], [1, 1, 0, 1]])\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGWXp1YN46Wq",
        "outputId": "d585f031-7359-49a2-81b3-e6751fe0a469"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 2]\n"
          ]
        }
      ]
    }
  ]
}